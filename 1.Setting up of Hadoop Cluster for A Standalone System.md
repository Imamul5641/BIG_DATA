## Setting Up Hadoop Cluster for a Standalone System

### Prerequisites
- **Update System:** Ensure your system is up-to-date.
  ```
  sudo apt update
  ```
  This will make sure that your system remains up-to-date.
  
- **Java Installation:** Java must be installed if it's not intstalled, install it using below commands.
  ```
  sudo apt install openjdk-8-jdk -y
  ```
  
- **Check the version using below commands** 
  ```
  java -version; javac -version
  ```
  
- **Secure Shell Setup:** Install Secure Shell (SSH) client and server.
  ```
  sudo apt install openssh-server openssh-client -y
  ```
  It will install all the dependencies
  
- **Create Hadoop User Group:** Create a Hadoop user group with sudo privileges.
```
sudo adduser {your_user}
```
change {your_name} to the any name you like. This will create a new user with the given name.
Fill all the information and remember the password you will give as it will the opening password for you added user shell.

- **Secure Shell Setup:** Now you switch the user from your primary interface to the interface you added.
 This will ensure that hadoop is using the given interface without any disturbance.
 By typing the below command in the terminal and entering the password for the shell
  ```
  su - {your_user}
  ```
  
  or directly changing from the start button by clicking on switch user.
  
  **If your user don't have sudouers priveleges:**
  
    - **Boot into Single-User Mode (Recovery Mode):**
        -Reboot the machine.
        -When the GRUB menu appears, select the "Advanced options for Ubuntu" and then choose the recovery mode for             the latest kernel.
        -Once in recovery mode, select the "Root Drop to root shell prompt" option.
      
    -**Remount File System as Read/Write:**
      ```
          mount -o remount,rw /
      ```
  
    -**Add User to sudo Group:**
      ```
        usermod -aG sudo hdoop_imamul
      ```
  
    -**Verify Changes:**
      ```
        id hdoop_imamul
      ```
  
        in my case it was hdoop_imamul for your it will be name of the user you created above in {your_user}.
  
    -**Exit Recovery Mode:**
      ```
        exit
      ```
  
    -**Reboot:**
      ```
        reboot
      ```
  
After the reboot, the changes should take effect, and the user "hdoop_imamul" in your case "{your_user}" should have sudo privileges.
  

### Installation Steps
1. **Generate SSH Key Pairs:**
    ```bash
    ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa
    ```

2. **Append Public Keys:**
    ```bash
    cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys
    ```

3. **Set Proper Permission:**
    ```bash
    chmod 0600 ~/.ssh/authorized_keys
    ```
    
3. **Verify SSH Authentication:**
    ```bash
    ssh localhost
    ```

4. **Download Hadoop:**
    ```bash
    wget https://dlcdn.apache.org/hadoop/common/hadoop-3.3.6/hadoop-3.3.6.tar.gz
    ```
    or directly going to the apache hadoop page and download the hadoop archive file.

5. **Extract Hadoop:**
    ```bash
    tar xzf hadoop-3.3.6.tar.gz
    ```
    you should change this version of hadoop-3.3.6.tar.gz to the version you have downloaded.    

### Configuration
1. **Java Environment Variables:**
    - Configure Java environment variables in `.bashrc`.
    - Activate the environment variables.

2. **Configure hadoop-env.sh:**
    ```bash
    sudo nano $HADOOP_HOME/etc/hadoop/hadoop-env.sh
    ```

3. **Configure core-site.xml:**
    Edit the configuration file `core-site.xml`.

4. **Configure hdfs-site.xml:**
    Edit the configuration file `hdfs-site.xml`.

5. **Configure yarn-site.xml:**
    Edit the configuration file `yarn-site.xml`.

6. **Configure mapred-site.xml:**
    Edit the configuration file `mapred-site.xml`.

### Directory Setup
- Make directories for HDFS, Namenode, and Datanode.

### Launching Hadoop
1. **Start DFS (Distributed File System):**
   Start the Hadoop cluster.
2. **Start YARN Service:**
   Start the YARN service.
3. **Check Installation:**
   Use `jps` command to verify the installation. Ensure services like SecondaryNameNode, NodeManager, ResourceManager, NameNode, and DataNode are running.

### Accessing Components
- Access Namenode and Resource Manager.
- Test by moving a local file to HDFS.
