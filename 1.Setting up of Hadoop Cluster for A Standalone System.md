## Setting Up Hadoop Cluster for a Standalone System

### Prerequisites
- **Update System:** Ensure your system is up-to-date.
  ```
  sudo apt update
  ```
  This will make sure that your system remains up-to-date.
  
- **Java Installation:** Java must be installed if it's not intstalled, install it using below commands.
  ```
  sudo apt install openjdk-8-jdk -y
  ```
  
- **Check the version using below commands** 
  ```
  java -version; javac -version
  ```
  
- **Secure Shell Setup:** Install Secure Shell (SSH) client and server.
  ```
  sudo apt install openssh-server openssh-client -y
  ```
  It will install all the dependencies
  
- **Create Hadoop User Group:** Create a Hadoop user group with sudo privileges.
```
sudo adduser {your_user}
```
change {your_name} to the any name you like. This will create a new user with the given name.
Fill all the information and remember the password you will give as it will the opening password for you added user shell.

- **Secure Shell Setup:** Now you switch the user from your primary interface to the interface you added.
 This will ensure that hadoop is using the given interface without any disturbance.
 By typing the below command in the terminal and entering the password for the shell
  ```
  su - {your_user}
  ```
  
  or directly changing from the start button by clicking on switch user.
  
  **If your user don't have sudouers priveleges:**
  
    - **Boot into Single-User Mode (Recovery Mode):**
        -Reboot the machine.
        -When the GRUB menu appears, select the "Advanced options for Ubuntu" and then choose the recovery mode for             the latest kernel.
        -Once in recovery mode, select the "Root Drop to root shell prompt" option.
      
    -**Remount File System as Read/Write:**
    ```bash
    mount -o remount,rw /
    ```
  
    -**Add User to sudo Group:**
    ```bash
    usermod -aG sudo hdoop_imamul
    ```
  
    -**Verify Changes:**
    ```bash
    id hdoop_imamul
    ```
    in my case it was hdoop_imamul for your it will be name of the user you created above in {your_user}.
  
    -**Exit Recovery Mode:**
    ```bash
    exit
    ```
  
    -**Reboot:**
    ```bash
    reboot
    ```
  
After the reboot, the changes should take effect, and the user "hdoop_imamul" in your case "{your_user}" should have sudo privileges.
  

### Installation Steps
1. **Generate SSH Key Pairs:**
    ```bash
    ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa
    ```

2. **Append Public Keys:**
    ```bash
    cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys
    ```

3. **Set Proper Permission:**
    ```bash
    chmod 0600 ~/.ssh/authorized_keys
    ```
    
3. **Verify SSH Authentication:**
    ```bash
    ssh localhost
    ```

4. **Download Hadoop:**
    ```bash
    wget https://dlcdn.apache.org/hadoop/common/hadoop-3.3.6/hadoop-3.3.6.tar.gz
    ```
    or directly going to the apache hadoop page and download the hadoop archive file.

5. **Extract Hadoop:**
    ```bash
    tar xzf hadoop-3.3.6.tar.gz
    ```
    you should change this version of hadoop-3.3.6.tar.gz to the version you have downloaded.    

### Configuration
1. **Java Environment Variables:**
    - Configure Java environment variables in `.bashrc`.
    ```bash
    sudo nano $HADOOP_HOME/etc/hadoop/hadoop-env.sh
    ```
    {your_user} should be changed to the name given by you
    ```nano
    #Add below lines in this file at the end  

    #Hadoop Related Options
    export HADOOP_HOME=/home/{your_user}/hadoop-3.2.3
    export HADOOP_INSTALL=$HADOOP_HOME
    export HADOOP_MAPRED_HOME=$HADOOP_HOME
    export HADOOP_COMMON_HOME=$HADOOP_HOME
    export HADOOP_HDFS_HOME=$HADOOP_HOME
    export YARN_HOME=$HADOOP_HOME
    export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native
    export PATH=$PATH:$HADOOP_HOME/sbin:$HADOOP_HOME/bin
    export HADOOP_OPTS"-Djava.library.path=$HADOOP_HOME/lib/nativ"
    ```
    - Activate the environment variables.
     ```bash
     source ~/.bashrc
     ```

2. **Configure hadoop-env.sh:**
    ```bash
    sudo nano $HADOOP_HOME/etc/hadoop/hadoop-env.sh
    ```
    ```nano
    #Add below line in this file in the end
    export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
    ```
3. **Configure core-site.xml:**
    Edit the configuration file `core-site.xml`.
    ```bash
    sudo nano $HADOOP_HOME/etc/hadoop/core-site.xml
    ```
    {your_name} should be changed to name given by you
    ```nano
    #Add below lines in this file(between "<configuration>" and "<"/configuration>")
     <property>
        <name>hadoop.tmp.dir</name>
        <value>/home/{your_name}/tmpdata</value>
        <description>A base for other temporary directories.</description>
     </property>
     <property>
        <name>fs.default.name</name>
        <value>hdfs://localhost:9000</value>
        <description>The name of the default file system></description>
     </property>
    ```

4. **Configure hdfs-site.xml:**
    Edit the configuration file `hdfs-site.xml`.
    ```bash
    sudo nano $HADOOP_HOME/etc/hadoop/hdfs-site.xml
    ```
    {your_name} should be changed to name given by you
    ```nano
    <property>
      <name>dfs.data.dir</name>
      <value>/home/{your_name}/dfsdata/namenode</value>
    </property>
    <property>
      <name>dfs.data.dir</name>
      <value>/home/{your_name}/dfsdata/datanode</value>
    </property>
    <property>
      <name>dfs.replication</name>
      <value>1</value>
    </property>
    ```
    
5. **Configure mapred-site.xml:**
    Edit the configuration file `mapred-site.xml`.
    ```bash
    sudo nano $HADOOP_HOME/etc/hadoop/mapred-site.xml
    ```
    ```nano
    #Add below lines in this file(between "<configuration>" and "<"/configuration>")

    <property>
      <name>mapreduce.framework.name</name>
      <value>yarn</value>
    </property>
    ```

8. **Configure yarn-site.xml:**
    Edit the configuration file `yarn-site.xml`.
    ```bash
    sudo nano $HADOOP_HOME/etc/hadoop/yarn-site.xml
    ```
    ```nano
    #Add below lines in this file(between "<configuration>" and "<"/configuration>")

    <property>
      <name>yarn.nodemanager.aux-services</name>
      <value>mapreduce_shuffle</value>
    </property>
    <property>
      <name>yarn.nodemanager.aux-services.mapreduce.shuffle.class</name>
      <value>org.apache.hadoop.mapred.ShuffleHandler</value>
    </property>
    <property>
      <name>yarn.resourcemanager.hostname</name>
      <value>127.0.0.1</value>
    </property>
    <property>
      <name>yarn.acl.enable</name>
      <value>0</value>
    </property>
    <property>
      <name>yarn.nodemanager.env-whitelist</name>
      <value>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PERPEND_DISTCACHE,
              HADOOP_YARN_HOME,HADOOP_MAPRED_HOME
      </value>
    </property>
    ```
### Launching Hadoop
```bash
hdfs namenode -format
./start-dfs.sh
```
### Directory Setup
- Make directories for HDFS, Namenode, and Datanode.

### Launching Hadoop
1. **Start DFS (Distributed File System):**
   Start the Hadoop cluster.
   ```bash
   start-dfs.sh
   ```
3. **Start YARN Service:**
   Start the YARN service.
   ```bash
   start-yarn.sh
   ```
5. **Check Installation:**
   Use `jps` command to verify the installation. Ensure services like SecondaryNameNode, NodeManager, ResourceManager, NameNode, and DataNode are running.
   ```bash
   jps
   ```

### Accessing Components
- Access Namenode and Resource Manager.
- Test by moving a local file to HDFS.
